%\documentclass[times, 11pt]{article}
\documentclass{acm_proc_article-sp}

%\usepackage{amsmath}
%\usepackage{amsfonts} % for mathbb
%\usepackage{algorithmic}
%\usepackage{algorithm}
%\usepackage{fullpage}
%\usepackage{amsthm}
%\usepackage{verbatim}
%\usepackage{hyperref}
%\usepackage{parskip}
%\usepackage{setspace}
%\usepackage{indentfirst}
\usepackage{color}
%\usepackage[english]{babel}
%\usepackage{url}
%\usepackage{graphicx}


%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{fact}[theorem]{Fact}
%\newtheorem{assumption}[theorem]{Assumption}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{prop}[theorem]{Proposition}
%\newtheorem{claim}[theorem]{Claim}
%\newtheorem{remark}[theorem]{Remark}
%\newtheorem{prob}[theorem]{Problem}
%\newtheorem{conjecture}[theorem]{Conjecture}
%\newtheorem{definition}[theorem]{Definition}

%\newcommand{\aline}{\centerline{\rule{6.5in}{0.1mm}}}


\newcommand{\secref}[1] {Section~\ref{sec-#1}}

\newcommand{\ignore}[1] {}

\begin{document}

\title{Agent-Based Modeling of User Circumvention of Security}
\subtitle{Position Paper}
%\date{}

\numberofauthors{2} 
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor
Vijay Kothari\\
       \affaddr{Department of Computer Science}\\
       \affaddr{Dartmouth College}\\
       \email{vijay.h.kothari.gr@dartmouth.edu}
% 2nd. author
\alignauthor
Jim Blythe\\
       \affaddr{Information Sciences Institute}\\
       \affaddr{University of Southern California}\\
       \email{blythe@isi.edu}
\and
% 3rd. author
\alignauthor Sean Smith\\
       \affaddr{Department of Computer Science}\\
       \affaddr{Dartmouth College}\\
       \email{sws@cs.dartmouth.edu}
\and  % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor Ross Koppel\\
       \affaddr{Department of Sociology}\\
       \affaddr{University of Pennsylvania}\\
       \email{rkoppel@sas.upenn.edu}
}

\maketitle

\setlength{\parindent}{1cm}
%\setlength{\parskip}{1cm minus5mm}
%\onehalfspacing
%\singlespacing
%\doublespacing

%\abstract

\begin{abstract}
Security subsystems are often designed with flawed assumptions arising from 
system designers' faulty mental models. 
Designers tend to assume that users behave according to some textbook ideal,
and to consider each potential exposure/interface in isolation.
However, fieldwork 
continually shows that even well-intentioned users often depart from this ideal
and circumvent controls in order to perform daily work tasks, and
that ``incorrect'' user behaviors can create unexpected links between 
otherwise ``independent" interfaces.
When it comes to security features and parameters,
designers try to find the choices that optimize security utility---except
these flawed assumptions give rise to an incorrect curve, and lead
to choices that actually make security worse, in practice.

We propose that improving this situation requires giving designers more
accurate models of real user behavior and how it influences aggregate system
security. Agent-based modeling can be a fruitful 
first step here.
In this paper, we study a
particular instance of this problem, propose
user-centric techniques designed to strengthen the security of systems
while simultaneously improving the usability of them, and propose
further directions of inquiry.
\end{abstract}

\section{Introduction}

At a relatively simple level, we can look at security as making a
design choice that optimizes some overall security goal (while staying
within economic constraints).  For example, a security officer
concerned with reducing the risk of some adversary logging into a
password-protected interface might choose to force users to have long,
complex, non-personally meaningful passwords which must be changed on
a regular basis.  In other words, the more the officer ``dials up''
her control knob, the more secure the system is.

However, fieldwork (e.g., \cite{blythe2013circumvention}, \cite{DP00},
\cite{florencio2007large}, \cite{gaw2006password},
\cite{riley2006password}) shows that human users behave in ways that
subvert this model.  For example, if a password is too complex, human
users will write it down (thus {\em increasing} compromise risk); if
forced to change a critical password, some users will change passwords
for other accounts (outside the officer's purview) to be the
same---thus increasing both risk {\em and} consequence of compromise.
Thus, we see a descent into an \textit{uncanny valley}: where
implementing a more stringent security mechanism may reduce the
overall security posture \cite{mori70}.
%
\ignore{Thus, what seems to be a simple monotonic relationship may in fact harbor
what graphics specialists call {\em uncanny valleys}: places where
dialing up the controls actually decreases overall security posture.}

Addressing this paradox requires understanding how and why user behavior
differs from ``optimum.''
People behave differently for a number of reasons:
%\begin{itemize}
%\item 
first, they may have a
different model of their environment from the security designers, with
typically a richer set of goals and action costs.
%\item 
Second, they may have a
different model of the security risks. The designer may typically have
a more accurate model but this is not always true.
%\item 
Third, even with
correct models, humans often make suboptimal choices, for example due
to biases in decision making, distractions, emotion and fatigue.
%\end{itemize}


Adams and Sasse 
\cite{adams1999users} challenged the belief that users 
are averse to engaging in secure behaviors and motivated the need for 
``user-centered design in security mechanisms.'' 
Through interviews, Beautement et al  \cite{beautement2009compliance} discovered that ``the key factors in 
the compliance decision are the actual and anticipated cost and benefits of 
compliance to the individual employee, and perceived cost and benefits to 
the organization.'' They also identified numerous examples of these
factors.
%\begin{itemize}
%\item 
Perceived costs of compliance included increased physical load,
increased cognitive load, embarrassment, missed opportunities and the `hassle factor'.
%\item 
Perceived benefits of compliance included avoiding the consequences of
a security breach and protection from sanctions.
%\item 
Finally, external factors of compliance included design, awareness,
training, and education, the culture of the organization, monitoring and sanctions.
%\end{itemize}
Furthermore, they postulated that users will comply when the task cost of 
compliance (individual cost - benefits) does not exceed the compliance threshold. 
Fieldwork also indicates security systems (e.g., anti-virus software, backup systems) 
often provide users with a false sense of security, allowing users to believe such 
systems act as safeguards against their own circumventions and security lapses.

Agent-based modeling can provide important insights into the question
of the optimal security posture by taking the potential human response
into account.  Some might classify circumvention as a \textit{wicked
  problem} (e.g., \cite{rittel1973dilemmas}) -- an extraordinarily
challenging problem that does not lend itself to definitive
formulation, which requires novel ideas and techniques to address.  We
argue that agent-based modeling can nonetheless provide significant
value.  Even when it is impossible to define \textit{a priori}, how users will
circumvent controls, agent-based simulations may be able to predict
when users would be inclined to circumvent given the opportunity,
e.g. by estimating frustration levels.  In some situations, we may be
able to address sources of frustration, and, in doing so, address the
problem of circumvention, without identifying a particular method of
circumvention. Furthermore, agent-based simulations may enable us to
look at not just the causes for surprising behavior, in the aggregate,
but also at the effects, in the aggregate.  In other situations, where
we can identify \textit{a posteriori}, a method of circumvention,
agent-based modeling may provide a deeper understanding of the factors
conducive to circumvention (such as the types of password misbehavior
documented in fieldwork surveys).


One can view the knowledge of a circumvention and the propensity to
use one as diffusing through a network of agents. In this case
agent-based simulations may help identify which population groups are
most susceptible to engaging in workarounds, and where the tipping
point is between between the occasional workaround and ubiquitous
circumvention.  Using this information, it may be possible to
re-structure a network of users to reduce the prevalence of
workarounds, e.g. by isolating groups or through education about the
organizational threats that are reduced by security mechanisms.
Additionally, agent-based simulations may help predict the efficacy of
countermeasures to circumvention without the costs and risks
associated with implementing such countermeasures in practice.


\ignore{Agent models of human behavior might be an effective way to better
predict the impact of a security system in a given environment. These
models may predict otherwise unexpected outcomes due to individual
circumvention or emergent effects when a group of users collaborate
over a network.}

In order to make effective predictions, an agent platform must capture
some of the behaviors that might be expected of human users. In
particular it should capture the mental models of end users where they
may differ from those of the system designers, and the potential
effects of known biases and of emotions such as frustration on user
compliance.  We are in the process of developing an agent platform
that captures these aspects. In this position paper we briefly present
the approach and describe an example where agent models may be used to
predict the best timeout value for automatically logging users off in
multi-user environments.  A model that assumes user compliance may
find that a short timeout is optimal, because it minimizes the chance
of another user either accessing material inappropriately or
accidentally entering information to the wrong account. However our preliminary
model predicts that a longer timeout may provide better performance in
an environment where the logout procedure can be actively
defeated---as actually and unexpectedly happened in a real-world
deployment at a partner institution.

In \secref{timeouts} we describe this scenario in more
detail. \secref{DASH} describes an agent-based approach that allows us
to explore the impact of workarounds and compliance on the optimal
design choices. \secref{future} discusses future work and other
domains in which to apply the approach. \secref{conclusion} concludes.

\section{An Anecdote Regarding Timeouts}
\label{sec-timeouts}

%%\textcolor{red}{HIPAA has automatic logoff requirements that are listed as ddressable... so perhaps recommendations here are a bit useless as the system designers really have little choice other than complying.}

In a partner institution (a large hospital), security officers were concerned
that {\em COWs} ({\em computers on wheels}) and desktop machines were 
too often left logged-in but unattended by clinicians, creating
the risk for confidentiality and integrity problems in stored patient data.

\ignore{In a partner institution (a large hospital),
clinicians used {\em COWs} ({\em computers on wheels}) and desktop machines.
The security officers were concerned
that these systems were too often left logged-in but unattended, creating
the risk for confidentiality and integrity problems in stored patient data.}

\ignore{To address this problem, the officers attached proximity sensors to the machines and added controls that,
when detecting that a computer had been left unattended but logged-in for a predetermined, fixed, period of time, the user would be automatically logged off.}
To address this problem, officers attached proximity sensors to the machines 
with controls that would automatically log off any user after a fixed 
duration of inactivity.
If the officers chose a timeout that was very short (say, 1 second),
the system would become unusable---massively frequent interruptions of workflow would cause users to noisily complain.  
However, beyond this short window, one would assume that longer timeouts
yield worse net exposure (e.g., total minutes of logged-in but unattended machines).   Anything between too-short and infinite would be an improvement 
on the original scenario.

What happened was unexpected: frustrated with the interruptions (and
perhaps with accuracy problems in the proximity detectors), some users
instead put styrofoam cups on the detectors, leading to all systems
always believing a user was present.  The naive designer model
suggested a monotonic decrease in net exposure as the timeout
increases; a more
%\ignore{linear} curve; a more
accurate model would require taking into account the aggregate effects
of user behavior, such as: (1) how frustrated different users might
get with timeouts perceived as too short; (2) how quickly frustrated
users might seek a workaround, such as the styrofoam cups; (3) how
many of the remaining users, who might not actively break the system
themselves, might happily use a system where someone else had left a
styrofoam cup; (4) whether having a security professional walking the
wards periodically and removing cups would actually improve things.
(How many patrols would be needed to make a real difference?)

\subsection{A Different Approach to the Timeout Decision}

Enabling designers to make better security decisions requires enabling
them to reason about these issues.  Our goal is to build an
agent-based model, as a first step away from the naive, incorrect
model. Such a system will allow designers to explore in simulation the
effects of different strategies, estimating the net benefits to
security and overall organizational efficiency, and considering
variables such as user frustration, to the extent they can be well
modeled.

\ignore{\subsection{An Alternative To The Standard Approach}}

The naive approach to timeouts uses a fixed timeout threshold and
neglects factors that are indicative of frustration experienced in the
event of a timeout. These factors, which include the intended use of the
system, fatigue, and stress, motivate choosing a timeout value that is
sensitive to the user's state, actions, and working environment rather
than choosing a constant. Given feedback, for example, a security tool
might learn to estimate the user's expected frustration caused by a
timeout based on the open applications on the computer, time of day,
and domain-specific indications of workload such as the patient
roster.  \ignore{At the same time it might learn to estimate the
  probability of a vulnerability caused by a lengthened timeout in the
  same way and use the two values to reason about the tradeoffs of
  user frustration and near-term security.}  It might also learn to
estimate the the probability that a very frustrated user might execute
an unforeseen workaround, and the probability that a less frustrated
user might copy such a workaround if she sees it.  Such a tool is
likely to require data about the operations where it is to
be deployed, but the parameters of interest and initial values could
be set by learning within the simulation.

\ignore{\subsection{Why Does It Take So Long To Recognize Workarounds?}}

\ignore{\textcolor{red}{this is all speculation... just because users don't report circumvention doesn't mean that system designers will be unaware of it. some solid real-world examples would be nice here-- an image may also be useful: users of system circumvent -> no need for users to complain -> system designers don't hear about it -> security folks' mental models of system remains the same, but the system itself is flawed-- and usage of the system does not mesh with the mental model}}

\ignore{There's rarely any incentive for users to report the annoying security measures of a system if those very measures can easily be circumvented without consequences. If users were to complain about such a system and/or mention employed workarounds, e.g. by help desk calls, it is likely that system designers will just make the next iteration of the system more robust to circumvention. This is supported by a dialogue we had with a security practitioner from industry who stated: ``when we as security practitioners become aware of the unchecked circumvention taking place, we respond by tightening our controls even further, by being even more intrusive in order to prevent circumvention.'' On the other hand, if system designers are kept out of the loop, just as users are often kept out of the loop during the design of the system, then users can merrily employ their workarounds and do their job with minimal frustration. The punchline is that users don't complain, system designers continue to hold their flawed mental models which have not been updated, the security of the system is compromised, and the security and organizational goals are not met.}

%\subsection{Security Objectives Should Not Be Considered In Isolation}

It is often counterproductive and in some situations even dangerous to
consider security objectives in isolation. Security mechanisms
implemented to realize security goals often impact other
organizational goals in a significant way that is often left
unaccounted for.  \ignore{For example, Koppel et al
  \cite{koppel2008workarounds} documented ways that circumvention
  relating to BCMA (Barcode Medication Administration) systems led to
  medical errors.}  However, the impact of security
  mechanisms on other organizational goals is not limited to scenarios
  involving workarounds. Even in the absence of workarounds, stringent
  security mechanisms can induce stress, fatigue, and changes in mood
  that impact workflow and hinder progress toward numerous
  organizational goals.

\ignore{Even in situations where 
workarounds are not employed, medical errors 
may arise due to sincere attempts by clinicians to comply with a system 
that interferes with workflow.}

In general, an optimal security strategy will depend on the interplay
between security objectives and organizational objectives, and between
different individuals in the organization. The complexity of the
problem is one motivating factor for an agent-based model of the
system to be secured, that captures the objectives of individual
agents and factors that influence their likelihood of compliance with
security protocols. We envision such a platform being employed by
security designers to test various security mechanisms when it is
infeasible to run real experiments for various reasons.

\ignore{Suppose we consider the most simple model in which users are incapable of circumvention and fixed timeout thresholds must be used. How do we set the right threshold? In order to even begin approaching this problem we must consider the interplay between security objectives and other organizational objectives. Additionally, we must incorporate the nuances of user behavior-- emotions, biases, etc. To this end, we are in the process of developing an agent-based platform. We envision such a platform being employed by security designers to test various security measures when it is infeasible to run real experiments due to ethical costs, monetary costs, or for other reasons.}

\ignore{\textcolor{red}{Here, I discuss the utility and argue in favor of modeling the whole environment and measuring organizational objectives rather than security objectives in isolation-- as Jim said, incorporating user frustration and emotion into the model will lead to such things as medical errors. When the objective function is organizational this may lead to different timeout thresholds.}}

\section{Modeling Workarounds}
\label{sec-DASH}

\ignore{\textcolor{red}{An argument against the utility of agent-based approaches that we should address: If we use an agent-based approach to predict whether a workaround will be employed we must recognize the existence of the workaround. Security designers, however, may not have even been aware of the existence of such a workaround (e.g. security designers may not have known that styrofoam cups could be placed over proximity sensors in order to trick the sensor into believing a clinician was nearby). Had they known of the workaround, perhaps they would have implemented safeguards to ensure the workaround was not employed. Modeling still of course has utility, but perhaps we should try to address this.}}

\ignore{\textcolor{red}{relating to timeouts, in circumvention of security: good users do bad things, the space bar pressing example could be incorporated in an agent model.}}

The agent-based platform we are developing builds on {\sc dash}, a
framework for modeling human agents \cite{blythe2012dual}. {\sc dash}
combines a dual-process approach with a BDI agent: at each time step,
an instinctive module may suggest a plausible action to be taken
directly by the agent or it may defer to a deliberative reasoning
module, which employs a BDI reactive planner to choose an action based
on explicit goals. The instinctive module maintains an activation
strength on nodes in memory that can be modified by an appraisal-based
emotion mechanism \cite{spraragen12}. This approach provides a natural
model for frustration, as a strong activation produced by a negative
appraisal of entities that are seen as conflicting with the agent's
plan. In field work, frustration has emerged as a significant factor
in the application of workarounds.

Regarding the styrofoam cup example, clinicians may perceive the
following as organizational and individual goals: minimizing medical
errors, ensuring patients are treated with dignity and attended to
promptly, minimizing unnecessary exposure of patient
data. \ignore{Individual goals may include things like providing
  superior patient care and finishing the current task adequately but
  quickly.}  These goals give rise to a utility function that can be
used to evaluate the perceived outcomes of executing plans, which is
done through the reasoning module. When using deliberative reasoning,
a {\sc dash} agent by default chooses among alternative actions by
projecting the plan associated with each one and picking the plan
whose outcome has highest utility according to its beliefs.

However, planning achieved through the reasoning module may also be
bypassed using the instinctive module when the agent is subject to
certain emotions such as frustration. For example, an agent under
stress caused by time pressure may skip a step to check that it is
logged into the computer rather than another agent, and proceed to
entering prescription information, since this more material step
receives high activation from the instinctive module. Since the
instinctive module provides input on goals as well as actions,
frustration may also impact action choices when the reasoning module
is employed. Suppose a clever user is frustrated with a policy
requiring routine password change. If the user is aware that the
system only prohibits the current password during a reset, and the
perceived burden of remembering a new password is sufficiently high,
she might create a plan to call the help desk after a reset so as to
have her old password restored. This matches behavior seen in
fieldwork.  Other attributes (e.g. fatigue, stress) may also have an
impact on the user's perception, beliefs, and mental processes.
 
We have implemented a prototype model as a first step to exploring the
interaction of security goals and user behavior in the timeout
scenario, and students in a special-topics course have built models
for other scenarios. We are now in the process of extending the models
to account for more of the behaviors found in fieldwork or by some of
the richer models available in {\sc dash}.

Validation is a significant challenge. In a two-step approach, we will
begin by showing that our models can duplicate behaviors seen in our and others'
field studies. Next, we are planning experiments to uncover and where
possible manipulate internal states such as frustration
(\cite{hazlett2003measurement}, \cite{kapoor2007automatic},
\cite{klein2002computer}, \cite{reynolds2001sensing}), stress and
fatigue, to both test hypotheses about the role they play and show
that our agent models can capture the behavior at a deeper level.

% some of the following papers may be enlightening
% Automatic prediction of frustration - Kapoor et al
% Measurement of User Frustration A Biologic Approach - Hazlett
% The Sensing and Measurement of Frustration with Computer - Carson Jonathan Reynolds
% This Computer Responds to User Frustration: Theory, Design, and Results - Klein et al

%%%%%%%%%%%%%%%%
% start ignore of original section 3
%%%%%%%%%%%%%%%%

\ignore{We have developed a model for the COW anecdote discussed earlier,
capable of running simple simulations, based on a subset of DASH. This
allows us to measure various security effects as a function of
variables in the model, the most pertinent being the timeout
threshold.

During each time step, each agent (clinician) begins or continues an action in
accordance with a short plan that requires the agent to be logged into a COW. 
During the simulation, agents may be timed out of a COW that
they had previously been using. These timeouts occur when users are
logged into a COW that they had not been using for a duration of time,
the timeout threshold.

Under normal operation, the agent will not interfere with the timeout
process and will log back in when necessary. However this action
increases the agent's frustration level, a parameter that is tracked
for this simulation and that decreases over time, unless an action
causes it to be increased. When the agent's frustration level reaches
a threshold, the agent may choose an alternative plan that defeats the
auto-logout sensor with a styrofoam cup, if the agent is aware of this
workaround. Some agents begin the simulation with this knowledge,
while we model other agents as gaining the knowledge by witnessing
another agent use the workaround.

%{\bf I tentatively replaced this text with the above.}
\ignore{\textcolor{red}{In each time step, simulation data and statistics are also updated. The two most important statistics that are measured are net exposure and average peak frustration. Net exposure is defined as the total duration of time for which clinicians are logged into COWs but not within close proximity to them. Peak frustration for a given clinician is defined as the highest level of experienced frustration for that clinician. Average peak frustration is computed by averaging peak frustrations over all clinicians.}}

\ignore{\textcolor{red}{Each clinician has a frustration value that changes when an action is carried out. Clinicians may be timed out of a COW that they had previously been using. These timeouts occur when users are logged into a COW that they had not been using for a duration of time, the timeout threshold. This results in the user experiencing further frustration the next time the user attempts an action that requires that COW. Users also experience a slight reduction in frustration every time step. If the user knows about the styrofoam cup workaround, the COW with which the user is interacting does not have a styrofoam cup placed over its proximity sensor, and a user's frustration exceeds her frustration threshold, she will employ said styrofoam cup workaround.}}

We have not yet performed real-world validation of this model. It is
described here as an implemented illustration of the agent modeling
approach. However, we intend to perform simple lab experiments to
validate the model by observing the correlation of timeout values and
other factors that increase frustration with the frequency with which
the workaround is employed.

In the future, we plan to build upon this model by incorporating
richer alternative plans, explicit appraisal and a more faithful model
of the way agents of different types interact in this environment.  We
would also like to implement a module that tracks the value of an
organizational objective function, dependent upon the medical errors,
monetary costs of plans, and security effects experienced during a
given simulation, instead of naively considering a security objective
in isolation.  We believe that we would arrive at a vastly different
timeout threshold if we optimize for organizational objectives instead
of solely optimizing for a security objective.}

\ignore{\textcolor{red}{
I swapped the paragraph given below with the previous  paragraph:
In the future, we plan to build upon this model by incorporating
richer alternative plans, explicit appraisal and a more faithful model
of the way agents of different types interact in this environment. It
would also be valuable to gather additional security and 
organizational statistics. Furthermore, we believe the model can be
used to explore an approach setting a timeout threshold that minimizes
some global objective function depending on variables such as
knowledge and ease of applying the workaround, monetary costs of
longer plans and of medical errors, frustration and other emotions,
user biases, etc. We believe such an approach would admit a vastly
different optimal solution than would be obtained by ignoring these
user aspects.
}}

%%%%%%%%%%%%%%%%
% end ignore of original section 3
%%%%%%%%%%%%%%%%

\section{Some Other Applications}
\label{sec-future}

In this section, we discuss future work and potential applications of
{\sc dash} to understanding other behavior that would be considered
undesirable from a security perspective along with other future
work. 

The approach we have described here is limited in that workarounds
must be explicitly described in the model in order to be used, and
therefore the simulation could never be used to predict workarounds
that are completely unexpected by the designers. It may be useful to
relax this assumption by allowing agents to search a space of plan
library modifications to find potential abstract workarounds. The
agent could then analyze the way in which the security protocol
reduced effectiveness of the agents plan and hypothesize an action or
a change to the effects of an action that would in turn defeat this
disruption. In the example of this paper, we might hypothesize that
agents may stop the timeout from taking effect if they can find a way
to nullify it that is less costly than making numerous
logins. Examples with more detailed security protocols might lead a
number of steps that might be open to attack.

As we discussed in the introduction, one promising avenue might be the
exploration of policies for password-based authentication. Fieldwork
(e.g., \cite{BS03, riley2006password, DP00, schechter2009s,
  schechter2007emperor, florencio2007large, gaw2006password}) provides
us a number of interesting user behaviors regarding passwords.  Most
users re-use a small number of unique passwords across a large number
of sites.  Some users circumvent password complexity rules by writing
down passwords.  Others circumvent by constructing passwords that are
all small modifications of each other.  Most users never change their
passwords unless they are forced to.  Many users cannot accurately
recall seldom-used passwords, at least within the first few guesses.
Many users have easily-guessed answers to security questions, even if
their passwords are strong.  Many users simultaneously know what good
password practices are, but fail to follow them.  Some users try to
choose stronger passwords when they perceive compromise of an account
might hurt them personally. Many users come up with 
the right passwords for the wrong usernames or for the wrong services.

What is the aggregate security effect of a decision regarding password
policy, if users behave as the surveys suggest in the proportions the
surveys suggest?  What shifts in user demographics (e.g., from law or
better training) might yield the best results?

Another set of avenues might be exploring behavior-based workarounds and 
errors in enterprise {\em authorization}
(e.g., \cite{SS08, Scout10, Scout08a})
Commercial enterprises tend to {\em over-entitlement}, as the perceived costs of under-entitlement are too high.
Enterprises also tend to over-entitlement, because users tend
to accumulate permissions over their career path (even keeping irrelevant ones across promotions and transfers).
Users may solve {\em under-entitlement} by circumventing the system completely---so the de facto access permitted by an enterprise's system may end up much larger than what the infosec managers perceive.
For security officers, the actual costs of under-entitlement---personally dealing with and assuaging angry users---may be much higher than the costs of over-entitlement.
Many managers provision new employees by copying-and-pasting 
entitlements from current employees, rather than thinking in detail.

One infosec officer at an investment bank reported that potential
clients would judge his bank's security by the question ``how many of
your employees will be able to see my data?''  Realistically
reasoning about this question, or the net amount of exposure, the
costs of under-entitlement, how much exposure could be reduced by
hiring $N$ more officers or switching to scheme $Y$, all requires 
modeling the aggregate behavior of humans.

\ignore{(Indeed, X.509 PKI has often been lamented as costing too many
man-hours when deployed in practice, in large enterprises.  Could our
approach help explain this, and then help guide designers to reduce
these costs?)}

\ignore{BCMA....}

\section{Conclusion}
\label{sec-conclusion}

In this paper, we have argued that to realize systems
security goals we must first fully understand the nuances of users and
user behavior.  We have also argued that we must stop looking at
security objectives in isolation --- incorrect assumptions by security
designers can have very real repercussions, e.g. due to circumvention,
that impact non-security goals. Agent-based modeling can help on both
fronts. In particular, we believe that the {\sc dash} framework can
assist system designers in understanding user behavior, predicting the
prevalence of workarounds, and measuring both security and
organizational benefits of various systems. We described a particular
scenario in timeouts and believe the agent-based approach will be
useful in a number of other applications, including password-based
authentication and authorization.

\section{Acknowledgements}
\label{sec-acks}

This material is based in part upon work support by the Army Research
Office under Award No. {\small W911NF-13-1-0086}.

\small

%\nocite{*}

\bibliographystyle{plain}
\bibliography{paper_refs}

\end{document} 
