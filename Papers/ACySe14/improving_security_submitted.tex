%\documentclass[times, 11pt]{article}
\documentclass{acm_proc_article-sp}

%\usepackage{amsmath}
%\usepackage{amsfonts} % for mathbb
%\usepackage{algorithmic}
%\usepackage{algorithm}
%\usepackage{fullpage}
%\usepackage{amsthm}
%\usepackage{verbatim}
%\usepackage{hyperref}
%\usepackage{parskip}
%\usepackage{setspace}
%\usepackage{indentfirst}
\usepackage{color}
%\usepackage[english]{babel}
%\usepackage{url}
%\usepackage{graphicx}


%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{fact}[theorem]{Fact}
%\newtheorem{assumption}[theorem]{Assumption}
%\newtheorem{corollary}[theorem]{Corollary}
%\newtheorem{prop}[theorem]{Proposition}
%\newtheorem{claim}[theorem]{Claim}
%\newtheorem{remark}[theorem]{Remark}
%\newtheorem{prob}[theorem]{Problem}
%\newtheorem{conjecture}[theorem]{Conjecture}
%\newtheorem{definition}[theorem]{Definition}

%\newcommand{\aline}{\centerline{\rule{6.5in}{0.1mm}}}


\newcommand{\secref}[1] {Section~\ref{sec-#1}}

\newcommand{\ignore}[1] {}

\begin{document}

\title{Agent-Based Modeling of User Circumvention of Security}
%\date{}

\maketitle

\setlength{\parindent}{1cm}
%\setlength{\parskip}{1cm minus5mm}
%\onehalfspacing
%\singlespacing
%\doublespacing

%\abstract

\begin{abstract}
Security subsystems are often designed with flawed assumptions arising from faulty
mental models held by system designers. 
Designers tend to assume that users behave according to some textbook ideal,
and to consider each potential exposure/interface in isolation.
However, fieldwork 
continually shows that even well-intentioned users often depart from this ideal
and circumvent controls in order to perform daily work tasks, and
that ``incorrect'' user behaviors can create unexpected links between 
otherwise ``independent" interfaces.
When it comes to security features and parameters,
designers try to find the choices that optimize security utility---except
these flawed assumptions give rise to an incorrect curve, and lead
to choices that actually make security worse, in practice.

We propose that improving this situation requires giving designers more
accurate models of real user behavior and how it influences aggregate system
security, and that agent-based modeling can be a fruitful 
first step here.
In this paper, we study a
particular instance of this problem, propose
user-centric techniques designed to strengthen the security of systems
while simultaneously improving the usability of them, and propose
further directions of inquiry.
\end{abstract}

\section{Introduction}

At a relatively simple level, we can look at security as
making a design choice that optimizes some overall security goal (while staying within economic constraints).   For example,
a security officer concerned with reducing the risk of some adversary
logging into a password-protected interface might choose to force users
to have long, complex, non-personally meaningful passwords which must be changed on a regular basis.  In other words, the more the officer ``dials up'' her control knob, the more secure the system is.     

However, fieldwork (e.g. \cite{blythe2013circumvention}, \cite{DP00},
\cite{florencio2007large}, \cite{gaw2006password},
\cite{riley2006password}) shows that human users behave in ways that
subvert this model.  For example, if a password is too complex, human
users will write it down (thus {\em increasing} compromise risk); if
forced to change a critical password, some users will change passwords
for other accounts (outside the officer's purview) to be the
same---thus increasing both risk {\em and} consequence of compromise.

%\textcolor{red}{what is the curve here? if it's just a general security vs knob curve, why assume it's a \textbf{\em linear} curve?}

Thus, what seems to be a simple monotonic relationship may in fact harbor
what graphics specialists call {\em uncanny valleys}: places where
dialing up the controls actually decreases overall security posture.

Solving this problem requires understanding how and why user behavior
differs from ``optimal.''

People behave differently for a number of reasons: (1) they have a
different model of their environment from the security designers, with
typically a richer set of goals and action costs, (2) they have a
different model of the security risks. The designer may typically have
a more accurate model but this is not always true, (3) even with
correct models, humans often make suboptimal choices, for example due
to biases in decision making, distractions, emotion and fatigue.

Agent models of human behavior might be an effective way to better
predict the impact of a security system in a given environment. These
models may predict otherwise unexpected outcomes due to individual
circumvention or emergent effects when a group of users collaborate
over a network. In order to make effective predictions, an agent
platform must capture some of the behaviors that might be expected of
human users. In particular it should capture the mental models of end
users where they may differ from those of the system designers, and
the potential effects of known biases and of emotions such as
frustration on user compliance.  Furthermore, agent-based simulations
may enable us to look at not just the causes for surprising behavior,
in the aggregate, but also at the effects, in the aggregate.

We are in the process of developing an agent platform that captures
these aspects. In this position paper we briefly present the approach
and describe an example where agent models are used to predict the
best timeout value for automatically logging users off in multi-user
environments.  A model that assumes user compliance may find that a
short timeout is optimal, because it minimizes the chance of another
user either accessing material inappropriately or accidentally
entering information to the wrong account. However our model predicts
that a longer timeout may provide better performance in an environment
where the logout procedure can be actively defeated---as actually and
unexpectedly happened in a real-world deployment at a partner
institution.

In \secref{timeouts} we describe this scenario in more
detail. \secref{DASH} describes an agent-based approach that allows us
to explore the impact of workarounds and compliance on the optimal
design choices. \secref{future} discusses future work and other
domains in which to apply the approach. \secref{conclusion} concludes.

\section{An Anecdote Regarding Timeouts}
\label{sec-timeouts}

%%\textcolor{red}{HIPAA has automatic logoff requirements that are listed as ddressable... so perhaps recommendations here are a bit useless as the system designers really have little choice other than complying.}


In a partner institution (a large hospital),
clinicians used {\em COWs} ({\em computers on wheels}) and desktop machines.
The security officers were concerned
that these systems were too often left logged-in but unattended, creating
the risk for confidentiality and integrity problems in the stored patient data.

To address this problem, the officers 
attached proximity sensors to the machines and added controls that,
when detecting that a computer had been left unattended but logged-in for a predetermined, fixed, period of time, the user would be automatically logged off.
If the officers chose a timeout that was very short (say, 1 second),
the system would become unusable---massively frequent interruptions of workflow would cause users to noisily complain.  
However, beyond this short window, one would assume that longer timeouts
yield worse net exposure (e.g., total minutes of logged-in but unattended machines).   Anything between too-short and infinite would be an improvement 
on the original scenario.

What happened was unexpected:
frustrated with the interruptions (and perhaps with accuracy problems
in the proximity detectors), some users instead put styrofoam cups on the 
detectors, leading to all systems always believing a user was present.
The naive designer model suggested a linear curve; a more accurate model
would require taking into account the aggregate effects of aggregate user  behavior, such as: (1) how frustrated different users might get with timeouts perceived as too-short; (2) how quickly such ``edgy'' users might find a workaround, such as the styrofoam cups; (3) how many of the remaining users who might not actively break the system themselves might happily use a system where someone else had left a styrofoam cup; (4) whether having a security intern walking the wards once a day and removing cups would actually improve things.  (How many interns would there have to be to make a real difference?)

\subsection{A Different Approach to the Timeout Decision}

Enabling designers to make better security decisions requires enabling
them to reason about these issues.  Our goal is to build an
agent-based model, as a first step away from the naive, incorrect
model. Such a system would allow designers to explore in simulation
the effects of different strategies, estimating the net benefits to
security and overall organizational efficiency, and considering
variables such as user frustration, to the extent they can be well
modeled.

%\subsection{An Alternative To The Standard Approach}

For example, the simple approach to timeouts uses a fixed timeout
threshold and neglects numerous factors that contribute to user
frustration in the event of a timeout. One indicative factor of
frustration experienced is the intended use of the system. A home
computer user who is timed out due to a lack of keyboard interaction
while watching a video on, say, NetFlix, is likely to experience a
greater level of frustration than one who is timed out while checking
the weather.

%Similarly, in a medical context, \textcolor{red}{[perhaps    Ross can provide a couple of solid examples here]}.

This motivates choosing a timeout value that is sensitive to the
user's actions and working environment rather than choosing a
constant. Given feedback, for example, a security tool might learn to
estimate the user's expected frustration caused by a timeout based on
the open applications on the computer, time of day, and
domain-specific indications of workload such as the patient roster. At
the same time it might learn to estimate the probability of a
vulnerability caused by a lengthened timeout in the same way and use
the two values to reason about the tradeoffs of user frustration and
near-term security. Such a tool is likely to require extensive data
about the operations where it is to be deployed, but the parameters of
interest and initial values could be set by learning within the
simulation.

\ignore{Envision a system where, whenever a user is going to leave a system unattended, she informs the system and identifies the action she will take. \ignore{\textcolor{red}{[this implicitly assumes that the user would identify an action, which needn't be the case!... perhaps an action can be assumed in such situation]}}. This action can then be used as input to an algorithm that determines an appropriate timeout threshold as a function of user attributes, the action specified, and other state. Perhaps such an approach can simultaneously achieve higher satisfaction ages among both security folks as well as users of the system in comparison to the traditional approach.}

\ignore{\subsection{Why Does It Take So Long To Recognize Workarounds?}}

\ignore{\textcolor{red}{this is all speculation... just because users don't report circumvention doesn't mean that system designers will be unaware of it. some solid real-world examples would be nice here-- an image may also be useful: users of system circumvent -> no need for users to complain -> system designers don't hear about it -> security folks' mental models of system remains the same, but the system itself is flawed-- and usage of the system does not mesh with the mental model}}

\ignore{There's rarely any incentive for users to report the annoying security measures of a system if those very measures can easily be circumvented without consequences. If users were to complain about such a system and/or mention employed workarounds, e.g. by help desk calls, it is likely that system designers will just make the next iteration of the system more robust to circumvention. This is supported by a dialogue we had with a security practitioner from industry who stated: ``when we as security practitioners become aware of the unchecked circumvention taking place, we respond by tightening our controls even further, by being even more intrusive in order to prevent circumvention.'' On the other hand, if system designers are kept out of the loop, just as users are often kept out of the loop during the design of the system, then users can merrily employ their workarounds and do their job with minimal frustration. The punchline is that users don't complain, system designers continue to hold their flawed mental models which have not been updated, the security of the system is compromised, and the security and organizational goals are not met.}

%\subsection{Security Objectives Should Not Be Considered In Isolation}

It is often counterproductive, and, in some situations, even
dangerous, to consider security objectives in isolation. Security
measures implemented to realize security goals often impact other
organizational goals in a significant way that is unaccounted for. For
example, Koppel et al \cite{koppel2008workarounds} demonstrated ways
that circumvention relating to BCMA (Barcode Medication
Administration) systems led to medical errors. Even in situations
where workarounds are not employed, medical errors may arise due to
sincere attempts by clinicians to comply with a system that interferes
with workflow.

In general, the optimal security strategy will depend on the interplay
between security objectives and organizational objectives, and between
different individuals in the organization, even in cases where
workarounds are not available. The complexity of the problem is one
motivating factor for an agent-based model of the system to be
secured, that captures the objectives of individual agents and factors
that influence their likelihood of compliance with security
protocols. We envision such a platform being employed by security
designers to test various security measures when it is infeasible to
run real experiments due to ethical costs, monetary costs, or for
other reasons.

\ignore{Suppose we consider the most simple model in which users are incapable of circumvention and fixed timeout thresholds must be used. How do we set the right threshold? In order to even begin approaching this problem we must consider the interplay between security objectives and other organizational objectives. Additionally, we must incorporate the nuances of user behavior-- emotions, biases, etc. To this end, we are in the process of developing an agent-based platform. We envision such a platform being employed by security designers to test various security measures when it is infeasible to run real experiments due to ethical costs, monetary costs, or for other reasons.}

\ignore{\textcolor{red}{Here, I discuss the utility and argue in favor of modeling the whole environment and measuring organizational objectives rather than security objectives in isolation-- as Jim said, incorporating user frustration and emotion into the model will lead to such things as medical errors. When the objective function is organizational this may lead to different timeout thresholds.}}

\section{Modeling Workarounds}
\label{sec-DASH}

\ignore{\textcolor{red}{An argument against the utility of agent-based approaches that we should address: If we use an agent-based approach to predict whether a workaround will be employed we must recognize the existence of the workaround. Security designers, however, may not have even been aware of the existence of such a workaround (e.g. security designers may not have known that styrofoam cups could be placed over proximity sensors in order to trick the sensor into believing a clinician was nearby). Had they known of the workaround, perhaps they would have implemented safeguards to ensure the workaround was not employed. Modeling still of course has utility, but perhaps we should try to address this.}}

\ignore{\textcolor{red}{relating to timeouts, in circumvention of security: good users do bad things, the space bar pressing example could be incorporated in an agent model.}}

The agent-based platform we are developing builds on {\sc dash}, a
framework for modeling human agents \cite{blythe2012dual}. {\sc dash}
combines a dual-process approach with a BDI agent: at each time step,
an instinctive module may suggest a plausible action to be taken
directly by the agent or it may defer to a deliberative reasoning
module, which employs a BDI reactive planner to choose an action based
on explicit goals. The instinctive module maintains an activation
strength on nodes in memory that can be modified by an appraisal-based
emotion mechanism \cite{spraragen12}. This approach provides a natural
model for frustration, as a strong activation produced by a negative
appraisal of entities that are seen as conflicting with the agent's
plan. In field work, frustration has emerged as a significant factor
in the application of workarounds.

We have developed a model for the COW anecdote discussed earlier,
capable of running simple simulations, based on a subset of DASH. This
allows us to measure various security effects as a function of
variables in the model, the most pertinent being the timeout
threshold.

During each time step, each agent (clinician) begins or continues an action in
accordance with a short plan that requires the agent to be logged in
to a COW. During the simulation, agents may be timed out of a COW that
they had previously been using. These timeouts occur when users are
logged into a COW that they had not been using for a duration of time,
the timeout threshold.

Under normal operation, the agent will not interfere with the timeout
process and will log back in when necessary. However this action
increases the agent's frustration level, a parameter that is tracked
for this simulation and that decreases over time, unless an action
causes it to be increased. When the agent's frustration level reaches
a threshold, the agent may choose an alternative plan that defeats the
auto-logout sensor with a styrofoam cup, if the agent is aware of this
workaround. Some agents begin the simulation with this knowledge,
while we model other agents as gaining the knowledge by witnessing
another agent use the workaround.



%{\bf I tentatively replaced this text with the above.}
\ignore{\textcolor{red}{In each time step, simulation data and statistics are also updated. The two most important statistics that are measured are net exposure and average peak frustration. Net exposure is defined as the total duration of time for which clinicians are logged into COWs but not within close proximity to them. Peak frustration for a given clinician is defined as the highest level of experienced frustration for that clinician. Average peak frustration is computed by averaging peak frustrations over all clinicians.}}

\ignore{\textcolor{red}{Each clinician has a frustration value that changes when an action is carried out. Clinicians may be timed out of a COW that they had previously been using. These timeouts occur when users are logged into a COW that they had not been using for a duration of time, the timeout threshold. This results in the user experiencing further frustration the next time the user attempts an action that requires that COW. Users also experience a slight reduction in frustration every time step. If the user knows about the styrofoam cup workaround, the COW with which the user is interacting does not have a styrofoam cup placed over its proximity sensor, and a user's frustration exceeds her frustration threshold, she will employ said styrofoam cup workaround.}}

We have not yet performed real-world validation of this model. It is
described here as an implemented illustration of the agent modeling
approach. However, we intend to perform simple lab experiments to
validate the model by observing the correlation of timeout values and
other factors that increase frustration with the frequency with which
the workaround is employed.

In the future, we plan to build upon this model by incorporating
richer alternative plans, explicit appraisal and a more faithful model
of the way agents of different types interact in this environment.  We
would also like to implement a module that tracks the value of an
organizational objective function, dependent upon the medical errors,
monetary costs of plans, and security effects experienced during a
given simulation, instead of naively considering a security objective
in isolation.  We believe that we would arrive at a vastly different
timeout threshold if we optimize for organizational objectives instead
of solely optimizing for a security objective.

\ignore{\textcolor{red}{
I swapped the paragraph given below with the previous paragraph:
In the future, we plan to build upon this model by incorporating
richer alternative plans, explicit appraisal and a more faithful model
of the way agents of different types interact in this environment. It
would also be valuable to gather additional security and 
organizational statistics. Furthermore, we believe the model can be
used to explore an approach setting a timeout threshold that minimizes
some global objective function depending on variables such as
knowledge and ease of applying the workaround, monetary costs of
longer plans and of medical errors, frustration and other emotions,
user biases, etc. We believe such an approach would admit a vastly
different optimal solution than would be obtained by ignoring these
user aspects.
}}

\section{Some Other Applications}
\label{sec-future}

In this section, we discuss future work and potential applications of
{\sc dash} to understanding other behavior that would be considered
undesirable from a security perspective along with other future
work. 

The approach we have described here is limited in that workarounds
must be explicitly described in the model in order to be used, and
therefore the simulation could never be used to predict workarounds
that are completely unexpected by the designers. It may be useful to
relax this assumption by allowing agents to search a space of plan
library modifications to find potential abstract workarounds. The
agent could then analyze the way in which the security protocol
reduced effectiveness of the agents plan and hypothesize an action or
a change to the effects of an action that would in turn defeat this
disruption. In the example of this paper, we might hypothesize that
agents may stop the timeout from taking effect if they can find a way
to nullify it that is less costly than making numerous
logins. Examples with more detailed security protocols might lead a
number of steps that might be open to attack.

As we discussed in the introduction, one promising avenue might be the
exploration of policies for password-based authentication. Fieldwork
(e.g., \cite{BS03, riley2006password, DP00, schechter2009s,
  schechter2007emperor, florencio2007large, gaw2006password}) provides
us a number of interesting user behaviors regarding passwords.  Most
users re-use a small number of unique passwords across a large number
of sites.  Some users circumvent password complexity rules by writing
down passwords.  Others circumvent by constructing passwords that are
all small modifications of each other.  Most users never change their
passwords unless they are forced to.  Many users cannot accurately
recall seldom-used passwords, at least within the first few guesses.
Many users have easily-guessed answers to security questions, even if
their passwords are strong.  Many users simultaneously know what good
password practices are, but fail to follow them.  Some users try to
choose stronger passwords when they perceive compromise of an account
might hurt them personally.

What is the aggregate security effect of a decision regarding password
policy, if users behave as the surveys suggest in the proportions the
surveys suggest?  What shifts in user demographics (e.g., from law or
better training) might yield the best results?

Another set of avenues might be exploring behavior-based workarounds and 
errors in enterprise {\em authorization}
(e.g., \cite{Scout08a, SS08, Scout10})
Commercial enterprises tend to {\em over-entitlement}, as the perceived costs of under-entitlement are too high.
Enterprises also tend to over-entitlement, because users tend
to accumulate permissions over their career path (even keeping irrelevant ones across promotions and transfers).
Users may solve {\em under-entitlement} by circumventing the system completely---so the de facto access permitted by an enterprise's system may end up much larger than what the infosec managers perceive.
For security officers, the actual costs of under-entitlement---personally dealing with and assuaging angry users---may be much higher than the costs of over-entitlement.
Many managers provision new employees by copying-and-pasting 
entitlements from current employees, rather than thinking in detail.

One infosec officer at an investment bank reported that potential
clients would judge his bank's security by the question ``how many of
your employees will be able to see my data?''.  Realistically
reasoning about this question, or the net amount of exposure, the
costs of under-entitlement, how much exposure could be reduced by
hiring $N$ more officers or switching to scheme $Y$, all requires the
model the aggregate behavior of humans.



\ignore{(Indeed, X.509 PKI has often been lamented as costing too many
man-hours when deployed in practice, in large enterprises.  Could our
approach help explain this, and then help guide designers to reduce
these costs?)}

\ignore{BCMA....}

\section{Conclusion}
\label{sec-conclusion}

In this paper, we have argued that in order to realize systems
security goals it is imperative that we first fully understand the
nuances of users and user behavior.  We have also argued that we must
stop looking at security objectives in isolation --- incorrect
assumptions by security designers can have very real repercussions,
e.g. due to circumvention, that impact non-security goals. Agent-based
modeling can help on both fronts. In particular, we believe that the
{\sc dash} framework can assist system designers in understanding user
behavior, predicting the prevalence of workarounds, and measuring both
security and organizational benefits of various systems. We have
studied a particular scenario in timeouts and believe the agent-based
approach will be useful in a number of other applications, including
password-based authentication and authorization.

\small

\nocite{*}

\bibliographystyle{plain}
\bibliography{paper_refs}

\end{document} 
