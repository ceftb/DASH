#!/usr/bin/python
import sys
import datetime
import gzip
import json
import cPickle as pickle
from glob import glob


def file_date_range(date1, date2):
    start = datetime.datetime.strptime(date1, '%Y%m%d')
    end = datetime.datetime.strptime(date2, '%Y%m%d')
    step = datetime.timedelta(days=1)
    dates = []
    while start < end:
        dates.append(start.date().strftime('%Y%m%d'))
        start += step

    dates.append(start.date().strftime('%Y%m%d'))
    return dates


def find_keywords(message, keywords):
    for phrase in keywords:
        if message.encode('utf-8').find(phrase) != -1:
            return True
    return False


def process_message(repo_id, user_id, event_type, users, repos):
    # init
    if repo_id not in repos:
        repos[repo_id] = {"tne": 0, "kne": 0, "pop": 0, "vln": 0} # tne - total number of events, kne - keyworded events, pop - popularity
    if user_id not in users:
        users[user_id] = {"tne": 0, "kne": 0, "idnt": 0.0, "intr": 0.0, "r":{}} # "r" map user to number of events of this user on each repo
    if repo_id not in users[user_id]["r"]:
        users[user_id]["r"][repo_id] = 0

    # total number of events
    repos[repo_id]["tne"] += 1
    users[user_id]["tne"] += 1
    users[user_id]["r"][repo_id] += 1

    # repo popularity
    if event_type == "WatchEvent" or event_type == "ForkEvent":
        repos[repo_id]["pop"] += 1


def process_keywords(message, repo_id, user_id, keywords, users, repos):
    # events with keywords
    if find_keywords(message, keywords):
        repos[repo_id]["kne"] += 1
        users[user_id]["kne"] += 1
        return True
    return False


def search_github_events(p_in_event, min_date, max_date, keywords):
    """
    Search for strings in github events
    :param p_in_event: The location of Github event files to load. eg: '/home/rcf-proj/ef/palashgo/SocialSim/data/socialsim/events/'
    :param min_date: min date. eg: '20170601'
    :param max_date: max date. eg: '20170801'
    :return:
    """

    ## collect needed files
    date_range = file_date_range(min_date, max_date)
    f_event = []
    for i in date_range:
        f_event += glob(p_in_event + i[0:6] + '/' + i + '/an_*.json.gz')

    ## parse event files
    event_counter = 0
    users = {}
    repos = {}
    for f_eve in f_event:
        with gzip.GzipFile(f_eve, 'r') as f_in:
            for line in f_in:
                d = json.loads(line)
                # process event
                event_counter += 1
                if event_counter % 10000 == 0:
                    print "processing event: ", event_counter
                if 'repo' in d and 'actor' in d and 'name_h' in d['repo'] and 'login_h' in d['actor']:
                    repo_id = d['repo']['name_h']
                    user_id = d['actor']['login_h']
                    event_type = d['type']
                    #if event_type == "PushEvent" or event_type == "IssueEvent":
                    process_message(repo_id, user_id, event_type, users, repos)
                    if "issue" in d["payload"]:  # if in issue comment
                        message = d["payload"]["issue"]["body_m"]
                        process_keywords(message, repo_id, user_id, keywords, users, repos)
                    elif "commits" in d["payload"]:  # if in commits (push event)
                        for commit in d["payload"]["commits"]:
                            message = commit["message_m"]
                            if process_keywords(message, repo_id, user_id, keywords, users, repos):
                                break

    # pass 2
    for user_id, user in users.iteritems():
        number_of_events_on_repos_with_vulnerabilities = 0
        for repo_id, counts in user["r"].iteritems():
            if repos[repo_id]["kne"] > 0:
                number_of_events_on_repos_with_vulnerabilities += counts
        users[user_id]["intr"] = float(number_of_events_on_repos_with_vulnerabilities) / float(users[user_id]["tne"])
        users[user_id]["idnt"] = float(users[user_id]["kne"]) / float(users[user_id]["tne"])

    for repo_id, repo in repos.iteritems():
        repos[repo_id]["vln"] = float(repos[repo_id]["kne"]) / float(repos[repo_id]["tne"])

    # save
    with open('users_augmentation.pickle', 'w') as u_out:
        for user_id, attributes in users.iteritems():
            data = {"id": user_id, "idnt": attributes["idnt"], "intr": attributes["intr"]}
            pickle.dump(data, u_out)
        u_out.close()

    sim_output_popularity = {}
    sim_output_vulnerability_event_counts = {}
    gt_file = open('repos_with_vulnerabilities_' + min_date + "-" + max_date + '_gt.txt', 'w')
    with open('repos_augmentation.pickle', 'w') as r_out:
        for repo_id, attributes in repos.iteritems():
            data = {"id": repo_id, "pop": attributes["pop"], "vln": attributes["vln"]}
            if attributes["kne"] > 0.0:
                gt_file.write(str(repo_id) + ", " + str(attributes["pop"]) + ", " + str(attributes["kne"]) + "\n")
                sim_output_vulnerability_event_counts[repo_id] = attributes["kne"]
                sim_output_popularity[repo_id] = attributes["pop"]
            pickle.dump(data, r_out)
        r_out.close()
    gt_file.close()

    f_p = open('repos_with_vulnerabilities_ev_dict_gt.pickle', 'w')
    pickle.dump(sim_output_vulnerability_event_counts, f_p)
    f_p.close()
    f_p = open('repos_with_vulnerabilities_pop_dict_gt.pickle', 'w')
    pickle.dump(sim_output_popularity, f_p)
    f_p.close()

    return


search_filter = ["vulnerability"]  # list of phrases to search
# search_filter = ["ecurity", "ulnerabilit", "attack", "emote code execution", "SQL injection", "sql injection", "XSS", "xss", "Cross Site Scripting", "ross site scripting", "ross site request", "data exposure"]
if __name__ == '__main__':
    search_github_events(sys.argv[1], sys.argv[2], sys.argv[3], search_filter)





