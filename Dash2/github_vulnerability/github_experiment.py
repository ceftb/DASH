import sys; sys.path.extend(['../../'])
import os
import time
import json
from datetime import datetime
import cPickle as pickle
from Dash2.core.measure import Measure
from Dash2.core.parameter import Parameter
from Dash2.core.experiment import Experiment
from Dash2.core.dash_controller import DashController
from Dash2.github_vulnerability.github_state_creator import GithubGraphBuilder, IdDictionaryInMemoryStream
from Dash2.socsim.network_utils import create_initial_state_files_from_raw_json
from Dash2.socsim.event_types import github_events_list, github_events
from Dash2.socsim.socsim_trial import SocsimTrial
from Dash2.socsim.socsim_work_processor import SocsimWorkProcessor
from Dash2.github_vulnerability.github_vulnerability_experiment_augmentation import search_github_events
from Dash2.github_vulnerability.evaluate import evaluate_predictions


class GithubWorkProcessor(SocsimWorkProcessor):
    module_name = "Dash2.github_vulnerability.github_experiment"

    def __init__(self, zk, host_id, task_full_id, data):
        SocsimWorkProcessor.__init__(self, zk, host_id, task_full_id, data)

    def process_after_run(self):
        SocsimWorkProcessor.process_after_run(self)
        sim_file = open(self.output_file_name +'_repos_with_vulnerabilities_simulation.txt', 'w')
        print "Printing repos with vulnerabilities"
        sim_output_popularity = {}
        sim_output_vulnerability_event_counts = {}
        for node_id in self.hub.graph.nodes:
            if self.hub.graph.nodes[node_id]["isU"] == 0 and "ve" in self.hub.graph.nodes[node_id] and self.hub.graph.nodes[node_id]["ve"] > 0:
                sim_file.write(str(self.hub.resource_ids[node_id]) + ", " + str(self.hub.graph.nodes[node_id]["pop"]) + "\n")
                sim_output_popularity[self.hub.resource_ids[node_id]] = self.hub.graph.nodes[node_id]["pop"]
                sim_output_vulnerability_event_counts[self.hub.resource_ids[node_id]] = self.hub.graph.nodes[node_id]["ve"]
        sim_file.close()

        #f_p = open(self.output_file_name + "_repos_with_vulnerabilities_ev_simulation.pickle", 'w')
        #pickle.dump(sim_output_vulnerability_event_counts, f_p)
        #f_p.close()
        f_p = open(self.output_file_name + '_repos_with_vulnerabilities_pop_simulation.pickle', 'w')
        pickle.dump(sim_output_popularity, f_p)
        f_p.close()

        os.remove(self.output_file_name + self.task_full_id + '_event_log_file.csv')



if __name__ == "__main__":
    experiment_name = sys.argv[1]
    domain = sys.argv[2]
    scenario = sys.argv[3]
    agent_class_name = sys.argv[4]  # "GithubAgent"
    agent_module_name = sys.argv[5]  # "Dash2.github.github_agent"
    start_date = sys.argv[6]
    end_date = sys.argv[7]
    output_file_name = sys.argv[8]
    events_path = sys.argv[9]
    input_events_start_date = sys.argv[10]
    input_events_end_date = sys.argv[11]
    input_events_vuln_training_start_date = sys.argv[12]
    input_events_vuln_training_end_date = sys.argv[13]
    keywords = str(sys.argv[14]).split(",")

    zk_hosts = '127.0.0.1:2181'
    number_of_hosts = 1
    number_of_days_in_simulation = 1.0 + float((time.mktime(datetime.strptime(str(end_date) + ' 00:00:00', "%Y%m%d %H:%M:%S").timetuple())
                                        - time.mktime(datetime.strptime(str(start_date) + ' 00:00:00', "%Y%m%d %H:%M:%S").timetuple())) / (3600.0 * 24.0))


    # Trial parameters and measures
    class AbstractTrial(SocsimTrial):
        parameters = [
            Parameter('start_time', default=time.mktime(
                datetime.strptime(str(start_date) + ' 00:00:00', "%Y%m%d %H:%M:%S").timetuple())),
            Parameter('max_time', default=time.mktime(
                datetime.strptime(str(end_date) + ' 23:59:59', "%Y%m%d %H:%M:%S").timetuple())),
            Parameter('hub_class_name', default="GithubHub"),
            Parameter('hub_module_name', default="Dash2.github_vulnerability.github_hub"),
            #Parameter('agent_class_name', default=""),
            Parameter('agent_module_name', default=agent_module_name),
            Parameter('embedding_path', default=""),
            Parameter('team_name', default="usc"),
            Parameter('scenario', default=scenario),
            Parameter('domain', default=domain),
            Parameter('platform', default="github")#,
            #Parameter('output_file_name', default=output_file_name)
        ]
        measures = [
            Measure('num_agents'),
            Measure('num_resources'),
            Measure('number_of_cross_process_communications'),
            Measure('memory_usage'),
            Measure('runtime')
        ]

        def process_after_run(self):  # merge log files from all workers
            SocsimTrial.process_after_run(self)
            output_file_name_ = self.output_file_name + "_trial_" + str(self.trial_id) + ".json"
            os.remove(output_file_name_)


    # Trial parameters and measures
    class GithubTrial_Base(AbstractTrial):
        parameters = []
        parameters.extend(AbstractTrial.parameters)
        parameters.append(Parameter('agent_class_name', default="GithubBaseAgent"))
        parameters.append(Parameter('output_file_name', default=output_file_name + "_base"))


    # Trial parameters and measures
    class GithubTrial_Reporter(AbstractTrial):
        parameters = []
        parameters.extend(AbstractTrial.parameters)
        parameters.append(Parameter('agent_class_name', default="GithubReporterAgent"))
        parameters.append(Parameter('output_file_name', default=output_file_name + "_reporter"))


    # Trial parameters and measures
    class GithubTrial(AbstractTrial):
        parameters = []
        parameters.extend(AbstractTrial.parameters)
        parameters.append(Parameter('agent_class_name', default=agent_class_name))
        parameters.append(Parameter('output_file_name', default=output_file_name))

        def process_after_run(self):  # merge log files from all workers
            AbstractTrial.process_after_run(self)
            # evaluation
            print "evaluating results ... "
            # vulnerability_experiment_repos_gt_popularity.pickle
            # vulnerability_experiment_repos_training_popularity.pickle
            # vulnerability_experiment_results_repos_with_vulnerabilities_pop_simulation.pickle
            evaluate_predictions(
                sim=[pickle.load(open(self.output_file_name + "_base_repos_with_vulnerabilities_pop_simulation.pickle", "r")),
                     pickle.load(open(self.output_file_name + "_reporter_repos_with_vulnerabilities_pop_simulation.pickle", "r")),
                     pickle.load(open(self.output_file_name + "_repos_with_vulnerabilities_pop_simulation.pickle", "r"))],
                gt=pickle.load(open(experiment_name + "_repos_gt_popularity.pickle", "r")),
                training=pickle.load(open(experiment_name + "_repos_training_popularity.pickle", "r")),
                total_number_of_repos=json.load(open(initial_state_file_name))["meta"]["number_of_resources"],
                output_file_name=self.output_file_name + "_evaluation_results")


    # if state file is not present, then create it. State file is created from input event log.
    # Users in the initial state are partitioned (number of hosts is the number of partitions)
    initial_state_file_name = str(experiment_name).split(".json")[0] + "_state.json"
    if not os.path.isfile(initial_state_file_name):

        # search_github_events() ground truth
        print "Creating ground truth (target set of relative repos) for vulnerability ..."
        search_github_events(events_path, start_date, end_date, keywords, experiment_name=experiment_name, target_set_only=True)

        # search_github_events()  initial state
        print "Creating training (user&repo augmentation and training) for vulnerability ..."
        search_github_events(events_path, input_events_vuln_training_start_date, input_events_vuln_training_end_date,
                             keywords, experiment_name=experiment_name, target_set_only=False)

        # initial state (graph builder)
        print "Creating initial state files. May take a while, please wait ..."
        create_initial_state_files_from_raw_json(experiment_name, events_path, input_events_start_date, input_events_end_date, GithubGraphBuilder,
                                                 github_events, github_events_list,
                                                 dictionary_stream_cls=IdDictionaryInMemoryStream,
                                                 initial_state_generators=None)
        print str(initial_state_file_name) + " file created."


    # experiment setup
    num_trials = 1
    experiment_data = {
        'max_iterations': number_of_days_in_simulation * 1110000 / number_of_hosts, # max_iterations_per_worker
        'initial_state_file': initial_state_file_name}

    controller = DashController(zk_hosts=zk_hosts, number_of_hosts=number_of_hosts)
    # trail Base
    exp = Experiment(trial_class=GithubTrial_Base,
                     work_processor_class=GithubWorkProcessor,
                     number_of_hosts=number_of_hosts,
                     exp_data=experiment_data,
                     num_trials=num_trials)
    controller.run(experiment=exp, run_data={}, start_right_away=True, continue_right_away=True)

    # trail Reporter
    exp = Experiment(trial_class=GithubTrial_Reporter,
                     work_processor_class=GithubWorkProcessor,
                     number_of_hosts=number_of_hosts,
                     exp_data=experiment_data,
                     num_trials=num_trials)
    controller.run(experiment=exp, run_data={}, start_right_away=True, continue_right_away=True)

    # trail 1
    exp = Experiment(trial_class=GithubTrial,
                     work_processor_class=GithubWorkProcessor,
                     number_of_hosts=number_of_hosts,
                     exp_data=experiment_data,
                     num_trials=num_trials)
    controller.run(experiment=exp, run_data={}, start_right_away=True, continue_right_away=False)
