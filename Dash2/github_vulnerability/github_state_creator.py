import sys; sys.path.extend(['../../'])
from Dash2.socsim.network_utils import GraphBuilder, create_initial_state_files, IdDictionaryInMemoryStream, IdDictionaryStream
from Dash2.socsim.event_types import github_events, github_events_list
import datetime
import gzip
import json
import cPickle as pickle
from glob import glob


def file_date_range(date1, date2):
    start = datetime.datetime.strptime(date1, '%Y%m%d')
    end = datetime.datetime.strptime(date2, '%Y%m%d')
    step = datetime.timedelta(days=1)
    dates = []
    while start < end:
        dates.append(start.date().strftime('%Y%m%d'))
        start += step

    dates.append(start.date().strftime('%Y%m%d'))
    return dates


class GithubGraphBuilder(GraphBuilder):

    def update_nodes_and_edges(self, user_id, resource_id, event_type, event_time, raw_json_event=None):
        GraphBuilder.update_nodes_and_edges(self, user_id, resource_id, event_type, event_time)
        # update node attributes: actionSubType and status
        #self.graph.edges[user_id, resource_id]["actionSubType"] = raw_json_event["actionSubType"]

    def finalize_graph(self):
        graph, number_of_users, number_of_resources = GraphBuilder.finalize_graph(self)

        # augmentation with probability of having/creating/identifying vulnerability
        user_augmentation_file = open(self.input_events_file_name + "_users_augmentation.pickle", "r")
        while True:
            try:
                user_augmentation_data = pickle.load(user_augmentation_file)
                if user_augmentation_data["id"] in self.user_id_dict.str_id_2_int_id:
                    int_user_id = self.user_id_dict.str_id_2_int_id[user_augmentation_data["id"]]
                    graph.nodes[int_user_id]["idnt"] = user_augmentation_data["idnt"]
                    graph.nodes[int_user_id]["intr"] = user_augmentation_data["intr"]
            except EOFError:
                break
        user_augmentation_file.close()

        repo_augmentation_file = open(self.input_events_file_name + "_repos_augmentation.pickle", "r")
        while True:
            try:
                repo_augmentation_data = pickle.load(repo_augmentation_file)
                if repo_augmentation_data["id"] in self.resource_id_dict.str_id_2_int_id:
                    int_repo_id = self.resource_id_dict.str_id_2_int_id[repo_augmentation_data["id"]]
                    graph.nodes[int_repo_id]["pop"] = repo_augmentation_data["pop"]
                    graph.nodes[int_repo_id]["vln"] = repo_augmentation_data["vln"]
                    graph.nodes[int_repo_id]["ve"] = 0
            except EOFError:
                break
        repo_augmentation_file.close()

        return graph, number_of_users, number_of_resources

    def build_graph_from_raw_json(self, events_path, min_date, max_date):
        min_date = str(min_date).replace("-","")
        max_date = str(max_date).replace("-", "")

        if self.graph is None:
            self.__init__(self.input_events_file_name, self.event_types, self.event_type_list)

        # read in events
        date_range = file_date_range(min_date, max_date)
        f_event = []
        for i in date_range:
            f_event += glob(events_path + i[0:6] + '/' + i + '/an_*.json.gz')

        # parse event files
        event_counter = 0
        for f_eve in f_event:
            with gzip.GzipFile(f_eve, 'r') as f_in:
                for line in f_in:
                    raw_event = json.loads(line)
                    # process event
                    event_counter += 1
                    if event_counter % 100000 == 0:
                        print "processing event: ", event_counter
                    if 'repo' in raw_event and 'actor' in raw_event and 'name_h' in raw_event['repo'] and 'login_h' in raw_event['actor']:
                        repo_id = raw_event['repo']['name_h']
                        user_id = raw_event['actor']['login_h']
                        event_type = raw_event['type']
                        event_time = raw_event['created_at']
                        event = {"actionType": event_type, "nodeTime": event_time, "nodeUserID": user_id, "nodeID": repo_id}
                        # process message
                        self.update_graph(event)

        graph, number_of_users, number_of_resources = self.finalize_graph()
        self.pickle_graph()
        self.clear_graph()

        return graph, number_of_users, number_of_resources

if __name__ == "__main__":
    filename = "../socsim/data_sample.json" #sys.argv[1]
    create_initial_state_files(filename, GithubGraphBuilder, github_events, github_events_list,
                               dictionary_stream_cls=IdDictionaryInMemoryStream,
                               initial_state_generators=None)
